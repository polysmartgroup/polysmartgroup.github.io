<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PolySmart Group </title> <meta name="author" content="PolySmart Group"> <meta name="description" content="A smart space at PolyU. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/polysmart_group_2.jpg?6c0b6dc8a7f2998e782d54829a297016"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://polysmartgroup.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/">Recruitment </a> </li> <li class="nav-item "> <a class="nav-link" href="/gain/">Gain </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> PolySmart Group </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/polysmart_head_img-480.webp 480w,/assets/img/polysmart_head_img-800.webp 800w,/assets/img/polysmart_head_img-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/polysmart_head_img.jpg?27cf2cc53e8a8a45eba2d1c5709fe8f2" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="polysmart_head_img.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>P504, Department of Computing,</p> <p>The Hong Kong Polytechnic University,</p> <p>Kowloon, Hong Kong</p> </div> </div> <div class="clearfix"> <p>A research and innovation group at <a href="https://www.polyu.edu.hk/" rel="external nofollow noopener" target="_blank">The Hong Kong Polytechnic University</a> (PolyU), directed by <a href="https://www.polyu.edu.hk/comp/people/academic-staff/prof-li-qing/" rel="external nofollow noopener" target="_blank">Prof. LI Qing</a>, <a href="https://www.polyu.edu.hk/comp/people/emeritus-honorary-adjunct-and-visiting/wei-xiaoyong---visiting/" rel="external nofollow noopener" target="_blank">Prof. WEI Xiaoyong</a> and <a href="https://www.zhangchen.info/" rel="external nofollow noopener" target="_blank">Prof. Chen Jason ZHANG</a>.</p> <p>Our group has been dedicated to diverse AI research fields, including Computer Vision, Multimedia, Natural Language Processing, AI for Science, Robotics, and AIoT.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 25, 2025</th> <td> Our work <a href="https://arxiv.org/pdf/2505.18581" rel="external nofollow noopener" target="_blank">Removal of Hallucination on Hallucination: Debate-Augmented RAG</a> is accepted by ACL 2025 Main. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 27, 2025</th> <td> Our work <a href="https://doi.org/10.1093/bib/bbaf194" rel="external nofollow noopener" target="_blank">GraphATC: advancing multilevel and multi-label anatomical therapeutic chemical classification via atom-level graph learning</a> is accepted by and published in Briefings in Bioinformatics. Congratulations to Wengyu for this achievement — this is his first research paper, completed during his undergraduate studies! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 24, 2025</th> <td> Congratulations to Yiyang and Wengyu on being awarded the prestigious Hong Kong PhD Fellowship Scheme (<a href="https://cerg1.ugc.edu.hk/hkpfs/index.html" rel="external nofollow noopener" target="_blank">HKPFS</a>)! We are excited to welcome them to continue their PhD journey in our lab. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 07, 2025</th> <td> A warm welcome to our <a href="https://polysmartgroup.github.io/team/">new student assistants Tianyi ZENG and Youkang WANG</a>! We are excited to meet you in the PolySmart group and look forward to working with you. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 23, 2024</th> <td> Warm welcome to our <a href="https://polysmartgroup.github.io/team/">new student assistants Haoming, Haoqian, Honghe, Yuhang and intern Tin Yeh</a>! We are excited to meet you in PolySmart group and look forward to working with you. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">ACL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drag_framework-480.webp 480w,/assets/img/publication_preview/drag_framework-800.webp 800w,/assets/img/publication_preview/drag_framework-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/drag_framework.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="drag_framework.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hu2025removal" class="col-sm-8"> <div class="title">Removal of Hallucination on Hallucination: Debate-Augmented RAG</div> <div class="author"> <em>Wentao Hu</em>, <em>Wengyu Zhang</em>, <em>Yiyang Jiang</em>, <em>Chen Jason Zhang</em>, <em>Xiaoyong Wei</em>, and <em>Qing Li</em> </div> <div class="periodical"> <em>In ACL 2025 Main</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.18581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Huenao/Debate-Augmented-RAG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://arxiv.org/pdf/2505.18581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hu2025removal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Removal of Hallucination on Hallucination: Debate-Augmented RAG}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Wentao and Zhang, Wengyu and Jiang, Yiyang and Zhang, Chen Jason and Wei, Xiaoyong and Li, Qing}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL 2025 Main}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">Briefings in Bioinformatics</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GraphATC_pipeline-480.webp 480w,/assets/img/publication_preview/GraphATC_pipeline-800.webp 800w,/assets/img/publication_preview/GraphATC_pipeline-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/GraphATC_pipeline.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GraphATC_pipeline.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2025graphatc" class="col-sm-8"> <div class="title">GraphATC: advancing multilevel and multi-label anatomical therapeutic chemical classification via atom-level graph learning</div> <div class="author"> <em>Wengyu Zhang</em>, Qi Tian, Yi Cao, Wenqi Fan, Dongmei Jiang, Yaowei Wang, <em>Qing Li</em>, and <em>Xiao-Yong Wei</em> </div> <div class="periodical"> <em>Briefings in Bioinformatics</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/lookwei/GraphATC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.1093/bib/bbaf194" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The accurate categorization of compounds within the anatomical therapeutic chemical (ATC) system is fundamental for drug development and fundamental research. Although this area has garnered significant research focus for over a decade, the majority of prior studies have concentrated solely on the Level 1 labels defined by the World Health Organization (WHO), neglecting the labels of the remaining four levels. This narrow focus fails to address the true nature of the task as a multilevel, multi-label classification challenge. Moreover, existing benchmarks like Chen-2012 and ATC-SMILES have become outdated, lacking the incorporation of new drugs or updated properties of existing ones that have emerged in recent years and have been integrated into the WHO ATC system. To tackle these shortcomings, we present a comprehensive approach in this paper. Firstly, we systematically cleanse and enhance the drug dataset, expanding it to encompass all five levels through a rigorous cross-resource validation process involving KEGG, PubChem, ChEMBL, ChemSpider, and ChemicalBook. This effort culminates in the creation of a novel benchmark termed ATC-GRAPH. Secondly, we extend the classification task to encompass Level 2 and introduce graph-based learning techniques to provide more accurate representations of drug molecular structures. This approach not only facilitates the modeling of Polymers, Macromolecules, and Multi-Component drugs more precisely but also enhances the overall fidelity of the classification process. The efficacy of our proposed framework is validated through extensive experiments, establishing a new state-of-the-art methodology. To facilitate the replication of this study, we have made the benchmark dataset, source code, and web server openly accessible.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025graphatc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Wengyu and Tian, Qi and Cao, Yi and Fan, Wenqi and Jiang, Dongmei and Wang, Yaowei and Li, Qing and Wei, Xiao-Yong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GraphATC: advancing multilevel and multi-label anatomical therapeutic chemical classification via atom-level graph learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{bbaf194}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1477-4054}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbaf194}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://academic.oup.com/bib/article-pdf/26/2/bbaf194/63012495/bbaf194.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/LLMEPET_demo-480.webp 480w,/assets/img/publication_preview/LLMEPET_demo-800.webp 800w,/assets/img/publication_preview/LLMEPET_demo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/LLMEPET_demo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="LLMEPET_demo.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="jiang2024prior" class="col-sm-8"> <div class="title">Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval</div> <div class="author"> <em>Yiyang Jiang</em>, <em>Wengyu Zhang</em>, <em>Xulu Zhang</em>, <em>Xiaoyong Wei</em>, Chang Wen Chen, and <em>Qing Li</em> </div> <div class="periodical"> <em>In ACM Multimedia 2024</em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.15051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/fletcherjiang/LLMEPET" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dl.acm.org/doi/10.1145/3664647.3681115" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this paper, we investigate the feasibility of leveraging large language models (LLMs) for integrating general knowledge and incorporating pseudo-events as priors for temporal content distribution in video moment retrieval (VMR) models. The motivation behind this study arises from the limitations of using LLMs as decoders for generating discrete textual descriptions, which hinders their direct application to continuous outputs like salience scores and inter-frame embeddings that capture inter-frame relations. To overcome these limitations, we propose utilizing LLM encoders instead of decoders. Through a feasibility study, we demonstrate that LLM encoders effectively refine inter-concept relations in multimodal embeddings, even without being trained on textual embeddings. We also show that the refinement capability of LLM encoders can be transferred to other embeddings, such as BLIP and T5, as long as these embeddings exhibit similar inter-concept similarity patterns to CLIP embeddings. We present a general framework for integrating LLM encoders into existing VMR architectures, specifically within the fusion module. The LLM encoder’s ability to refine concept relation can help the model to achieve a balanced understanding of the foreground concepts (e.g., persons, faces) and background concepts (e.g., street, mountains) rather focusing only on the visually dominant foreground concepts. Additionally, we introduce the concept of pseudo-events, obtained through event detection techniques, to guide the prediction of moments within event boundaries instead of crossing them, which can effectively avoid the distractions from adjacent moments. The integration of semantic refinement using LLM encoders and pseudo-event regulation is designed as plug-in components that can be incorporated into existing VMR methods within the general framework. Through experimental validation, we demonstrate the effectiveness of our proposed methods by achieving state-of-the-art performance in VMR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2024prior</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Yiyang and Zhang, Wengyu and Zhang, Xulu and Wei, Xiaoyong and Chen, Chang Wen and Li, Qing}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Multimedia 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BDoG_demo-480.webp 480w,/assets/img/publication_preview/BDoG_demo-800.webp 800w,/assets/img/publication_preview/BDoG_demo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/BDoG_demo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="BDoG_demo.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zheng2024a" class="col-sm-8"> <div class="title">A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning</div> <div class="author"> Changmeng Zheng, DaYong Liang, <em>Wengyu Zhang</em>, <em>Xiaoyong Wei</em>, Tat-Seng Chua, and <em>Qing Li</em> </div> <div class="periodical"> <em>In ACM Multimedia 2024</em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.14972" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/thecharm/BDoG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dl.acm.org/doi/10.1145/3664647.3681102" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2024a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zheng, Changmeng and Liang, DaYong and Zhang, Wengyu and Wei, Xiaoyong and Chua, Tat-Seng and Li, Qing}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Multimedia 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GAL_demo-480.webp 480w,/assets/img/publication_preview/GAL_demo-800.webp 800w,/assets/img/publication_preview/GAL_demo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/GAL_demo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GAL_demo.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2024generative" class="col-sm-8"> <div class="title">Generative Active Learning for Image Synthesis Personalization</div> <div class="author"> <em>Xulu Zhang</em>, <em>Wengyu Zhang</em>, <em>Xiaoyong Wei</em>, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, and <em>Qing Li</em> </div> <div class="periodical"> <em>In ACM Multimedia 2024</em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.14987" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/zhangxulu1996/GAL4Personalization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dl.acm.org/doi/10.1145/3664647.3680773" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>This paper presents a pilot study that explores the application of active learning, traditionally studied in the context of discriminative models, to generative models. We specifically focus on image synthesis personalization tasks. The primary challenge in conducting active learning on generative models lies in the open-ended nature of querying, which differs from the closed form of querying in discriminative models that typically target a single concept. We introduce the concept of anchor directions to transform the querying process into a semi-open problem. We propose a direction-based uncertainty sampling strategy to enable generative active learning and tackle the exploitation-exploration dilemma. Extensive experiments are conducted to validate the effectiveness of our approach, demonstrating that an open-source model can achieve superior performance compared to closed-source models developed by large companies, such as Google’s StyleDrop. The source code is available at https://github.com/zhangxulu1996/GAL4Personalization</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024generative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generative Active Learning for Image Synthesis Personalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xulu and Zhang, Wengyu and Wei, Xiaoyong and Wu, Jinlin and Zhang, Zhaoxiang and Lei, Zhen and Li, Qing}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Multimedia 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Compositional-Inversion-480.webp 480w,/assets/img/publication_preview/Compositional-Inversion-800.webp 800w,/assets/img/publication_preview/Compositional-Inversion-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Compositional-Inversion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Compositional-Inversion.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2024compositional" class="col-sm-8"> <div class="title">Compositional inversion for stable diffusion models</div> <div class="author"> <em>Xulu Zhang</em>, <em>Xiao-Yong Wei</em>, Jinlin Wu, <em>Tianyi Zhang</em>, Zhaoxiang Zhang, Zhen Lei, and <em>Qing Li</em> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28565" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/zhangxulu1996/Compositional-Inversion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Inversion methods, such as Textual Inversion, generate personalized images by incorporating concepts of interest provided by user images. However, existing methods often suffer from overfitting issues, where the dominant presence of inverted concepts leads to the absence of other desired concepts. It stems from the fact that during inversion, the irrelevant semantics in the user images are also encoded, forcing the inverted concepts to occupy locations far from the core distribution in the embedding space. To address this issue, we propose a method that guides the inversion process towards the core distribution for compositional embeddings. Additionally, we introduce a spatial regularization approach to balance the attention on the concepts being composed. Our method is designed as a post-training approach and can be seamlessly integrated with other inversion methods. Experimental results demonstrate the effectiveness of our proposed approach in mitigating the overfitting problem and generating more diverse and balanced compositions of concepts in the synthesized images. The source code is available at https://github.com/zhangxulu1996/Compositional-Inversion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024compositional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compositional inversion for stable diffusion models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xulu and Wei, Xiao-Yong and Wu, Jinlin and Zhang, Tianyi and Zhang, Zhaoxiang and Lei, Zhen and Li, Qing}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://ojs.aaai.org/index.php/AAAI/article/view/28565}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7350--7358}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%73%30%30%37.%77%65%69@%70%6F%6C%79%75.%65%64%75.%68%6B" title="email"><i class="fa-solid fa-envelope"></i></a> </div> <div class="contact-note">The best way to reach us is by email to Prof. Xiaoyong WEI.</div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 PolySmart Group. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-team",title:"Team",description:"Members of PolySmart Group.",section:"Navigation",handler:()=>{window.location.href="/team/"}},{id:"nav-publications",title:"Publications",description:"Publications in PolySmart Group.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"Projects in PolySmart Group.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-recruitment",title:"Recruitment",description:"",section:"Navigation",handler:()=>{window.location.href="/recruitment/"}},{id:"nav-gain",title:"Gain",description:"What can you gain from PolySmart Group?",section:"Navigation",handler:()=>{window.location.href="/gain/"}},{id:"nav-contact",title:"Contact",description:"",section:"Navigation",handler:()=>{window.location.href="/contact/"}},{id:"news-our-work-lt-a-href-quot-https-arxiv-org-pdf-2505-18581-quot-gt-removal-of-hallucination-on-hallucination-debate-augmented-rag-lt-a-gt-is-accepted-by-acl-2025-main",title:"Our work &lt;a href=&quot;https://arxiv.org/pdf/2505.18581&quot;&gt;Removal of Hallucination on Hallucination: Debate-Augmented RAG&lt;/a&gt; is accepted by ACL 2025 Main.",description:"",section:"News"},{id:"news-our-work-lt-a-href-quot-https-doi-org-10-1093-bib-bbaf194-quot-gt-graphatc-advancing-multilevel-and-multi-label-anatomical-therapeutic-chemical-classification-via-atom-level-graph-learning-lt-a-gt-is-accepted-by-and-published-in-briefings-in-bioinformatics-congratulations-to-wengyu-for-this-achievement-this-is-his-first-research-paper-completed-during-his-undergraduate-studies",title:"Our work &lt;a href=&quot;https://doi.org/10.1093/bib/bbaf194&quot;&gt;GraphATC: advancing multilevel and multi-label anatomical therapeutic chemical classification via atom-level graph learning&lt;/a&gt; is accepted by and published in Briefings in Bioinformatics. Congratulations to Wengyu for this achievement \u2014 this is his first research paper, completed during his undergraduate studies!",description:"",section:"News"},{id:"news-congratulations-to-yiyang-and-wengyu-on-being-awarded-the-prestigious-hong-kong-phd-fellowship-scheme-lt-a-href-quot-https-cerg1-ugc-edu-hk-hkpfs-index-html-quot-gt-hkpfs-lt-a-gt-we-are-excited-to-welcome-them-to-continue-their-phd-journey-in-our-lab",title:"Congratulations to Yiyang and Wengyu on being awarded the prestigious Hong Kong PhD Fellowship Scheme (&lt;a href=&quot;https://cerg1.ugc.edu.hk/hkpfs/index.html&quot;&gt;HKPFS&lt;/a&gt;)! We are excited to welcome them to continue their PhD journey in our lab.",description:"",section:"News"},{id:"news-a-warm-welcome-to-our-lt-a-href-quot-https-polysmartgroup-github-io-team-quot-gt-new-student-assistants-tianyi-zeng-and-youkang-wang-lt-a-gt-we-are-excited-to-meet-you-in-the-polysmart-group-and-look-forward-to-working-with-you",title:"A warm welcome to our &lt;a href=&quot;https://polysmartgroup.github.io/team/&quot;&gt;new student assistants Tianyi ZENG and Youkang WANG&lt;/a&gt;! We are excited to meet you in the PolySmart group and look forward to working with you.",description:"",section:"News"},{id:"news-warm-welcome-to-our-lt-a-href-quot-https-polysmartgroup-github-io-team-quot-gt-new-student-assistants-haoming-haoqian-honghe-yuhang-and-intern-tin-yeh-lt-a-gt-we-are-excited-to-meet-you-in-polysmart-group-and-look-forward-to-working-with-you",title:"Warm welcome to our &lt;a href=&quot;https://polysmartgroup.github.io/team/&quot;&gt;new student assistants Haoming, Haoqian, Honghe, Yuhang and intern Tin Yeh&lt;/a&gt;! We are excited to meet you in PolySmart group and look forward to working with you.",description:"",section:"News"},{id:"news-our-work-lt-a-href-quot-https-arxiv-org-abs-2403-14972-quot-gt-a-picture-is-worth-a-graph-a-blueprint-debate-paradigm-for-multimodal-reasoning-lt-a-gt-has-been-nominated-for-lt-a-href-quot-https-2024-acmmm-org-best-paper-quot-gt-best-paper-awards-in-acm-multimedia-2024-lt-a-gt",title:"Our work &lt;a href=&quot;https://arxiv.org/abs/2403.14972&quot;&gt;A Picture Is Worth a Graph - A Blueprint Debate Paradigm for Multimodal Reasoning&lt;/a&gt; has been nominated for &lt;a href=&quot;https://2024.acmmm.org/best-paper&quot;&gt;Best Paper Awards in ACM Multimedia 2024&lt;/a&gt;.",description:"",section:"News"},{id:"news-lt-a-href-quot-https-polysmartgroup-github-io-recruitment-quot-gt-we-are-recruiting-student-assistants-lt-a-gt",title:"&lt;a href=&quot;https://polysmartgroup.github.io/recruitment/&quot;&gt;We are recruiting student assistants!&lt;/a&gt;",description:"",section:"News"},{id:"news-our-work-lt-a-href-quot-https-arxiv-org-abs-2407-15051-quot-gt-prior-knowledge-integration-via-llm-encoding-and-pseudo-event-regulation-for-video-moment-retrieval-lt-a-gt-is-accepted-by-acm-multimedia-2024-oral",title:"Our work &lt;a href=&quot;https://arxiv.org/abs/2407.15051&quot;&gt;Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval&lt;/a&gt; is accepted by ACM Multimedia 2024 (Oral).",description:"",section:"News"},{id:"news-our-work-lt-a-href-quot-https-arxiv-org-abs-2403-14987-quot-gt-generative-active-learning-for-image-synthesis-personalization-lt-a-gt-is-accepted-by-acm-multimedia-2024",title:"Our work &lt;a href=&quot;https://arxiv.org/abs/2403.14987&quot;&gt;Generative Active Learning for Image Synthesis Personalization&lt;/a&gt; is accepted by ACM Multimedia 2024.",description:"",section:"News"},{id:"news-our-work-lt-a-href-quot-https-arxiv-org-abs-2403-14972-quot-gt-a-picture-is-worth-a-graph-a-blueprint-debate-paradigm-for-multimodal-reasoning-lt-a-gt-is-accepted-by-acm-multimedia-2024-oral",title:"Our work &lt;a href=&quot;https://arxiv.org/abs/2403.14972&quot;&gt;A Picture Is Worth a Graph - A Blueprint Debate Paradigm for Multimodal Reasoning&lt;/a&gt; is accepted by ACM Multimedia 2024 (Oral).",description:"",section:"News"},{id:"news-we-are-keeping-building-the-website",title:"We are keeping building the website!",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-introducing-the-homepage-of-polysmart-group",title:"Introducing the homepage of PolySmart Group!",description:"",section:"News"},{id:"members-youkang-wang",title:"Youkang WANG",description:"",section:"Members",handler:()=>{window.location.href="/members/albert"}},{id:"members-prof-chen-jason-zhang",title:"Prof. Chen Jason ZHANG",description:"",section:"Members",handler:()=>{window.location.href="/members/chen-jason-zhang"}},{id:"members-chenang-jiang",title:"Chenang JIANG",description:"",section:"Members",handler:()=>{window.location.href="/members/chenang-jiang"}},{id:"members-haoqian-du",title:"Haoqian DU",description:"",section:"Members",handler:()=>{window.location.href="/members/duhao-qian"}},{id:"members-honghe-ding",title:"Honghe DING",description:"",section:"Members",handler:()=>{window.location.href="/members/honghe-ding"}},{id:"members-jiangshan-he",title:"Jiangshan HE",description:"",section:"Members",handler:()=>{window.location.href="/members/jiangshan-he"}},{id:"members-dr-jiaxin-wu",title:"Dr. Jiaxin WU",description:"",section:"Members",handler:()=>{window.location.href="/members/jiaxin-wu"}},{id:"members-jinhao-shen",title:"Jinhao SHEN",description:"",section:"Members",handler:()=>{window.location.href="/members/jinhao-shen"}},{id:"members-prof-qing-li",title:"Prof. Qing LI",description:"",section:"Members",handler:()=>{window.location.href="/members/qing-li"}},{id:"members-rubing-chen",title:"Rubing Chen",description:"",section:"Members",handler:()=>{window.location.href="/members/rubing-chen"}},{id:"members-tianyi-zeng",title:"Tianyi ZENG",description:"",section:"Members",handler:()=>{window.location.href="/members/tianyi-zeng"}},{id:"members-tianyi-zhang",title:"Tianyi ZHANG",description:"",section:"Members",handler:()=>{window.location.href="/members/tianyi-zhang"}},{id:"members-tin-yeh-huang",title:"Tin Yeh HUANG",description:"",section:"Members",handler:()=>{window.location.href="/members/tinyeh-haung"}},{id:"members-wengyu-zhang",title:"Wengyu ZHANG",description:"",section:"Members",handler:()=>{window.location.href="/members/wengyu-zhang"}},{id:"members-wentao-hu",title:"Wentao HU",description:"",section:"Members",handler:()=>{window.location.href="/members/wentao-hu"}},{id:"members-prof-xiaoyong-wei",title:"Prof. Xiaoyong WEI",description:"",section:"Members",handler:()=>{window.location.href="/members/xiaoyong-wei"}},{id:"members-xulu-zhang",title:"Xulu ZHANG",description:"",section:"Members",handler:()=>{window.location.href="/members/xulu-zhang"}},{id:"members-yiyang-jiang",title:"Yiyang JIANG",description:"",section:"Members",handler:()=>{window.location.href="/members/yiyang-jiang"}},{id:"members-haoming-yuan",title:"Haoming YUAN",description:"",section:"Members",handler:()=>{window.location.href="/members/yuanhao-ming"}},{id:"members-yuhang-dai",title:"Yuhang DAI",description:"",section:"Members",handler:()=>{window.location.href="/members/yuhang-dai"}},{id:"projects-ai-coffee-machine",title:"AI Coffee Machine",description:"An innovation that recommends, makes, and serves the perfect coffee tailored to your preferences and health needs",section:"Projects",handler:()=>{window.location.href="/projects/ai_coffee_machine/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%73%30%30%37.%77%65%69@%70%6F%6C%79%75.%65%64%75.%68%6B","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>